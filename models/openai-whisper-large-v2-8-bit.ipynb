{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Automation Speech Recognition Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade transformers datasets accelerate timm datasets[audio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet evaluate jiwer mistral-common bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup ugging Face\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from torchinfo import summary\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "main_path = Path(\"/kaggle/working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_whisper(model, processor, sample):\n",
    "    result = {\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"reference\": normalize_text(sample[\"text\"]),\n",
    "    }\n",
    "\n",
    "    audio = sample[\"audio\"]\n",
    "    inputs = processor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(model.device, dtype=torch.float16)\n",
    "\n",
    "    outputs = model.generate(**inputs, do_sample=False)\n",
    "    predicted_ids = outputs[0]\n",
    "    transcription = processor.decode(predicted_ids)\n",
    "\n",
    "    result[\"prediction\"] = normalize_text(transcription)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def benchmark(model, processor, predict_fn, dataset, max_samples: int | None = None):\n",
    "    benchmark_results = []\n",
    "    wer = load(\"wer\")\n",
    "\n",
    "    for i, sample in enumerate(tqdm(dataset, desc=\"Benchmarking\", total=max_samples)):\n",
    "        if max_samples is not None and i >= max_samples:\n",
    "            break\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        prediction = predict_fn(model, processor, sample)\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        inference_time = end_time - start_time\n",
    "        wer_result = 100 * wer.compute(\n",
    "            references=[prediction[\"reference\"]],\n",
    "            predictions=[prediction[\"prediction\"]],\n",
    "        )\n",
    "\n",
    "        benchmark_results.append(\n",
    "            {**prediction, \"inference_time\": inference_time, \"wer\": wer_result}\n",
    "        )\n",
    "\n",
    "    total_samples = len(benchmark_results)\n",
    "    average_inference_time = np.mean(\n",
    "        [result[\"inference_time\"] for result in benchmark_results]\n",
    "    ).item()\n",
    "    average_wer = np.mean([result[\"wer\"] for result in benchmark_results]).item()\n",
    "\n",
    "    benchmark_summary = {\n",
    "        \"total_samples\": total_samples,\n",
    "        \"average_inference_time\": average_inference_time,\n",
    "        \"average_wer\": average_wer,\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        \"benchmark_results\": benchmark_results,\n",
    "        \"benchmark_summary\": benchmark_summary,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_results(result, folder_path: Path):\n",
    "    benchmark_results = pd.DataFrame(result[\"benchmark_results\"])\n",
    "    benchmark_summary = result[\"benchmark_summary\"]\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    benchmark_results.to_csv(folder_path / \"benchmark_results.csv\", index=False)\n",
    "\n",
    "    with open(folder_path / \"benchmark_summary.json\", \"w\") as f:\n",
    "        json.dump(benchmark_summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"test\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openai/whisper-large-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "whisper_model_8_bit = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"openai/whisper-large-v2\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "summary(whisper_model_8_bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = benchmark(\n",
    "    whisper_model_8_bit, whisper_processor, predict_whisper, dataset, max_samples=1000\n",
    ")\n",
    "output_folder = main_path / \"openai/whisper-large-v2/8-bit\" \n",
    "save_results(results, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
