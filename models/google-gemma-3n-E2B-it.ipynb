{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9039c0",
   "metadata": {},
   "source": [
    "# Benchmarking Automation Speech Recognition Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d945722-cafe-48bf-b24f-741ffd710de4",
   "metadata": {},
   "source": [
    "## Kaggle Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f896c-5829-4b80-b4fb-17c0d6e8dbe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T14:48:35.113967Z",
     "iopub.status.busy": "2025-07-30T14:48:35.113590Z",
     "iopub.status.idle": "2025-07-30T14:51:39.042029Z",
     "shell.execute_reply": "2025-07-30T14:51:39.041114Z",
     "shell.execute_reply.started": "2025-07-30T14:48:35.113948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade transformers datasets accelerate timm datasets[audio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0eb9b8-fd3f-403e-9182-a17cf48cd001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T14:51:39.043428Z",
     "iopub.status.busy": "2025-07-30T14:51:39.043122Z",
     "iopub.status.idle": "2025-07-30T14:51:45.604378Z",
     "shell.execute_reply": "2025-07-30T14:51:45.603698Z",
     "shell.execute_reply.started": "2025-07-30T14:51:39.043392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --quiet evaluate jiwer mistral-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71392ce9-8879-45bd-9d09-91eb35309c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T14:51:45.606338Z",
     "iopub.status.busy": "2025-07-30T14:51:45.606115Z",
     "iopub.status.idle": "2025-07-30T14:51:46.497007Z",
     "shell.execute_reply": "2025-07-30T14:51:46.496276Z",
     "shell.execute_reply.started": "2025-07-30T14:51:45.606317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup ugging Face\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf894d-7468-4d2d-acfd-a54d8641f1a8",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d563e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T15:01:06.821791Z",
     "iopub.status.busy": "2025-07-30T15:01:06.821092Z",
     "iopub.status.idle": "2025-07-30T15:01:06.827977Z",
     "shell.execute_reply": "2025-07-30T15:01:06.827175Z",
     "shell.execute_reply.started": "2025-07-30T15:01:06.821760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from torchinfo import summary\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoProcessor, Gemma3nForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94653f7b",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bdcf6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T14:52:09.117901Z",
     "iopub.status.busy": "2025-07-30T14:52:09.117682Z",
     "iopub.status.idle": "2025-07-30T14:52:09.121652Z",
     "shell.execute_reply": "2025-07-30T14:52:09.120981Z",
     "shell.execute_reply.started": "2025-07-30T14:52:09.117883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "main_path = Path(\"/kaggle/working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff2cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T14:52:09.122712Z",
     "iopub.status.busy": "2025-07-30T14:52:09.122475Z",
     "iopub.status.idle": "2025-07-30T14:52:09.141334Z",
     "shell.execute_reply": "2025-07-30T14:52:09.140720Z",
     "shell.execute_reply.started": "2025-07-30T14:52:09.122696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_gemma(model, processor, sample):\n",
    "    result = {\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"reference\": normalize_text(sample[\"text\"]),\n",
    "    }\n",
    "\n",
    "    audio = sample[\"audio\"]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are an assistant that transcribes speech accurately.\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": audio[\"array\"]},\n",
    "                {\"type\": \"text\", \"text\": \"Please transcribe this audio.\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device, dtype=torch.bfloat16)\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "\n",
    "    outputs = model.generate(**inputs, do_sample=False)\n",
    "    predicted_ids = outputs[0, input_len:]\n",
    "    transcription = processor.decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "    result[\"prediction\"] = transcription.lower()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e4fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T14:52:09.142439Z",
     "iopub.status.busy": "2025-07-30T14:52:09.142156Z",
     "iopub.status.idle": "2025-07-30T14:52:09.157558Z",
     "shell.execute_reply": "2025-07-30T14:52:09.156916Z",
     "shell.execute_reply.started": "2025-07-30T14:52:09.142415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def benchmark(model, processor, predict_fn, dataset, max_samples: int | None = None):\n",
    "    benchmark_results = []\n",
    "    wer = load(\"wer\")\n",
    "\n",
    "    for i, sample in enumerate(tqdm(dataset, desc=\"Benchmarking\", total=max_samples)):\n",
    "        if max_samples is not None and i >= max_samples:\n",
    "            break\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        prediction = predict_fn(model, processor, sample)\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        inference_time = end_time - start_time\n",
    "        wer_result = 100 * wer.compute(\n",
    "            references=[prediction[\"reference\"]],\n",
    "            predictions=[prediction[\"prediction\"]],\n",
    "        )\n",
    "\n",
    "        benchmark_results.append(\n",
    "            {**prediction, \"inference_time\": inference_time, \"wer\": wer_result}\n",
    "        )\n",
    "\n",
    "    total_samples = len(benchmark_results)\n",
    "    average_inference_time = np.mean(\n",
    "        [result[\"inference_time\"] for result in benchmark_results]\n",
    "    ).item()\n",
    "    average_wer = np.mean([result[\"wer\"] for result in benchmark_results]).item()\n",
    "\n",
    "    benchmark_summary = {\n",
    "        \"total_samples\": total_samples,\n",
    "        \"average_inference_time\": average_inference_time,\n",
    "        \"average_wer\": average_wer,\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        \"benchmark_results\": benchmark_results,\n",
    "        \"benchmark_summary\": benchmark_summary,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7efae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T14:52:09.158503Z",
     "iopub.status.busy": "2025-07-30T14:52:09.158273Z",
     "iopub.status.idle": "2025-07-30T14:52:09.171984Z",
     "shell.execute_reply": "2025-07-30T14:52:09.171373Z",
     "shell.execute_reply.started": "2025-07-30T14:52:09.158475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_results(result, folder_path: Path):\n",
    "    benchmark_results = pd.DataFrame(result[\"benchmark_results\"])\n",
    "    benchmark_summary = result[\"benchmark_summary\"]\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    benchmark_results.to_csv(folder_path / \"benchmark_results.csv\", index=False)\n",
    "\n",
    "    with open(folder_path / \"benchmark_summary.json\", \"w\") as f:\n",
    "        json.dump(benchmark_summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df11b4d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9bde5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T14:52:09.174649Z",
     "iopub.status.busy": "2025-07-30T14:52:09.174454Z",
     "iopub.status.idle": "2025-07-30T14:52:13.598288Z",
     "shell.execute_reply": "2025-07-30T14:52:13.597514Z",
     "shell.execute_reply.started": "2025-07-30T14:52:09.174628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"test\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b719c",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b79f3",
   "metadata": {},
   "source": [
    "### google/gemma-3n-E2B-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a94a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T15:00:36.136985Z",
     "iopub.status.busy": "2025-07-30T15:00:36.136728Z",
     "iopub.status.idle": "2025-07-30T15:00:39.645570Z",
     "shell.execute_reply": "2025-07-30T15:00:39.644051Z",
     "shell.execute_reply.started": "2025-07-30T15:00:36.136962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gemma3n_processor = AutoProcessor.from_pretrained(\"google/gemma-3n-e2b-it\")\n",
    "gemma3n_model = Gemma3nForConditionalGeneration.from_pretrained(\n",
    "    \"google/gemma-3n-e2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281af99d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-30T15:00:39.646157Z",
     "iopub.status.idle": "2025-07-30T15:00:39.646485Z",
     "shell.execute_reply": "2025-07-30T15:00:39.646297Z",
     "shell.execute_reply.started": "2025-07-30T15:00:39.646282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "summary(gemma3n_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55f0fa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-30T15:00:39.647772Z",
     "iopub.status.idle": "2025-07-30T15:00:39.648089Z",
     "shell.execute_reply": "2025-07-30T15:00:39.647929Z",
     "shell.execute_reply.started": "2025-07-30T15:00:39.647913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ddc3b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-30T15:00:39.648980Z",
     "iopub.status.idle": "2025-07-30T15:00:39.649273Z",
     "shell.execute_reply": "2025-07-30T15:00:39.649112Z",
     "shell.execute_reply.started": "2025-07-30T15:00:39.649096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = benchmark(\n",
    "    gemma3n_model,\n",
    "    gemma3n_processor,\n",
    "    predict_gemma,\n",
    "    dataset,\n",
    "    max_samples=10,\n",
    ")\n",
    "output_folder = main_path / \"google/gemma-3n-e2b-it\"\n",
    "save_results(results, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
